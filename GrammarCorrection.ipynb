{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "788v8HudNUTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade language_tool_python"
      ],
      "metadata": {
        "id": "Urq-0jbtbHLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9e5gN_QNL3w",
        "outputId": "a4d05945-fd03-4119-a189-b4e0454c9647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> You:How are you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: I'm good. How are you?\n",
            ">> You:correct my grammer\n",
            ">> You:my name as Seher\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: my name as Seher\n",
            "Text after correction: My name as Sewer\n",
            ">> You:correct my grammer\n",
            ">> You:I are student\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I are student\n",
            "Text after correction: I am student\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "import language_tool_python \n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "######################### DO NOT CHANGE#########################\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "######################### DO NOT CHANGE#########################\n",
        "\n",
        "## for dongusunu silmeyin ##\n",
        "## Do not remove the for loop ##\n",
        "for step in range(5):\n",
        "\n",
        "\n",
        "    ### Burada, cümleyi kullanıcıdan yazarak aliyoruz ama biz mikrofondan alacağız sonra tekrar metne donusturecegiz\n",
        "    ### Here,sentence is taken from user by typing, but we will take it from microphone and then convert it to text\n",
        "    text = input(\">> You:\")\n",
        "    if text==\"correct my grammer\":\n",
        "      text = input(\">> You:\")\n",
        "      \n",
        "      ######################### DO NOT CHANGE#########################\n",
        "      input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "      bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n",
        "      chat_history_ids = model.generate(\n",
        "          bot_input_ids,\n",
        "          max_length=1000,\n",
        "          do_sample=True,\n",
        "          top_p=0.95,\n",
        "          top_k=0,\n",
        "          temperature=0.75,\n",
        "          pad_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "      ######################### DO NOT CHANGE#########################\n",
        "    \n",
        "\n",
        "      ### Kullanici gramer duzeltme islemini belirtmedi ise, bu output degismeyecek\n",
        "      ### If the user did not specify the grammar correction command, this is our output \n",
        "      output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "      ### Kullanici \"correct my grammar\" dedi ise eger, gramer duzeltme islemini yapacagiz ve islemin sonucu output'umuz olacak\"\n",
        "      ### when the user says \"correct my grammar\", we will do the grammar correction and the result of this operation will be our output\"\n",
        "      \n",
        "      my_tool = language_tool_python.LanguageTool('en-US') \n",
        "      my_text = text\n",
        "      correct_text = my_tool.correct(my_text)  \n",
        "      \n",
        "      # printing some texts  \n",
        "      print(\"Original Text:\", my_text)  \n",
        "      print(\"Text after correction:\", correct_text) \n",
        "    else:\n",
        "      ######################### DO NOT CHANGE#########################\n",
        "      input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "      bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n",
        "      chat_history_ids = model.generate(\n",
        "          bot_input_ids,\n",
        "          max_length=1000,\n",
        "          do_sample=True,\n",
        "          top_p=0.95,\n",
        "          top_k=0,\n",
        "          temperature=0.75,\n",
        "          pad_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "      ######################### DO NOT CHANGE#########################\n",
        "    \n",
        "\n",
        "      ### Kullanici gramer duzeltme islemini belirtmedi ise, bu output degismeyecek\n",
        "      ### If the user did not specify the grammar correction command, this is our output \n",
        "      output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "      #### Output'u metin olarak değil, ses olarak cikartacagiz\n",
        "      #### We will display the output as a voice not as a text \n",
        "      print(f\"DialoGPT: {output}\")"
      ]
    }
  ]
}